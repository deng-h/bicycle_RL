## 24/10/30
- 完成checkpoints Wrapper和getCameraImage方法优化
- 没有训练成果的一天
## 24/10/31
- 优化了getCameraImage方法
- 把特征提取和智能体的训练分离，这样可以加快智能体的训练，[文章](https://arxiv.org/pdf/1901.08651#page=1.29)
- 提供过往的actions和states,解决通信延时的方法之一是提供过往的actions和states，把这些一起作为输入
## 24/11/02
- 引入RL Zoo框架，修改自行车模型代码(主要是设置服务器的ID)
- 理解了RL Zoo框架的基本结构
## 24/11/03
- 把特征提取和智能体的训练分离，这样可以加快智能体的训练，
[这篇文章](https://arxiv.org/pdf/1901.08651#page=1.29)
主要用的是state representation learning(SRL)方法，
但是我看网上说这种方法对于on-policy方法不适用
## 24/11/04
- 理解框架的plot部分，感觉和他的项目深度耦合，不如自己写plot方法
## 24/11/05
- 增加PPO网络的层数和神经元数，2层->3层，256个神经元->512个神经元
- 修改了平衡奖励函数的计算方式，使得奖励函数更加平滑
- 能够移动到目标点，但是会撞墙(需要设计奖励函数)
- 训练时的环境数适度降低
## 24/11/06
- 添加碰撞检测奖励函数
- 优化奖励函数的奖惩机制，目前自行车能平衡运动，但是不能到达目标点，且会撞墙
- 增加训练次数试试
## 24/11/08
- 让自行车拥有后退功能，这样可以更好避免撞墙
- 增加碰撞次数限制，如果碰撞次数超过限制，就会被重置，同时增加了碰撞次数的惩罚
## 24/11/11
- 到达目标点的奖励函数不合理，需要优化
## 24/11/12
- 现在自行车只会随意走，不去目标点，一直在蹭分，需要设计惩罚机制
## 25/02/07
- 从昨天开始写论文，摆了将近两个月，再不写就毕不了业了
- 这两天写完平衡控制的仿真实验部分
- 争取在开学前写完导航的初稿
- 加油！！！
## 25/02/08
- 平衡控制的仿真实验部分收尾
- 写完了作者简历与致谢，开始审查第二章内容
- 今天完成了第三章《硬件与软件系统设计》章节的建立，完成了课题背景与意义内容的写作
## 25/02/09
- 今天任务是完善第二章内容（已完成）
- 明天任务把5.2PPO网络改进实现搬到第四章，整理完善第四章内容